{
  "paragraphs": [
    {
      "text": "print(s\"\"\"%html\n\u003ccenter\u003e\n    \u003ch1\u003e\u003ca href\u003d\"http://diplodatos.famaf.unc.edu.ar/\"\u003eDiplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\u003c/a\u003e\u003c/h1\u003e\n    \u003ch2\u003eCurso \u003ca href\u003d\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\"\u003eProgramación Distribuida sobre Grandes Volúmenes de Datos\u003c/a\u003e\u003c/h2\u003e\n\u003c/center\u003e\n\n\u003cbr\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e Damián Barsotti  \u003c/h3\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    Facultad de Matemática Astronomía Física y Computación\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ca href\u003d\"http://www.unc.edu.ar\"\u003e\n    Universidad Nacional de Córdoba\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ccenter\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    \u003cimg src\u003d\"$baseDir/comun/logo%20UNC%20FAMAF%202016.png\" alt\u003d\"Drawing\" style\u003d\"width:50%;\"/\u003e\n    \u003c/a\u003e\n    \u003c/center\u003e\n\u003c/h3\u003e\n\n\u003cp style\u003d\"font-size:15px;\"\u003e\n    \u003cbr /\u003e\n        This work is licensed under a\n        \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\u003c/a\u003e.\n    \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003e\n        \u003cimg alt\u003d\"Creative Commons License\" style\u003d\"border-width:0;vertical-align:middle;float:right\" src\u003d\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /\u003e\n    \u003c/a\u003e\n\u003c/p\u003e\n\"\"\")\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.300",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003ccenter\u003e\n    \u003ch1\u003e\u003ca href\u003d\"http://diplodatos.famaf.unc.edu.ar/\"\u003eDiplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\u003c/a\u003e\u003c/h1\u003e\n    \u003ch2\u003eCurso \u003ca href\u003d\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\"\u003eProgramación Distribuida sobre Grandes Volúmenes de Datos\u003c/a\u003e\u003c/h2\u003e\n\u003c/center\u003e\n\n\u003cbr\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e Damián Barsotti  \u003c/h3\u003e\n\n\u003ch3 style\u003d\"text-align:center;\"\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    Facultad de Matemática Astronomía Física y Computación\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ca href\u003d\"http://www.unc.edu.ar\"\u003e\n    Universidad Nacional de Córdoba\n    \u003c/a\u003e\n\u003cbr/\u003e\n    \u003ccenter\u003e\n    \u003ca href\u003d\"http://www.famaf.unc.edu.ar\"\u003e\n    \u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/comun/logo%20UNC%20FAMAF%202016.png\" alt\u003d\"Drawing\" style\u003d\"width:50%;\"/\u003e\n    \u003c/a\u003e\n    \u003c/center\u003e\n\u003c/h3\u003e\n\n\u003cp style\u003d\"font-size:15px;\"\u003e\n    \u003cbr /\u003e\n        This work is licensed under a\n        \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003eCreative Commons Attribution-NonCommercial-ShareAlike 4.0 International License\u003c/a\u003e.\n    \u003ca rel\u003d\"license\" href\u003d\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"\u003e\n        \u003cimg alt\u003d\"Creative Commons License\" style\u003d\"border-width:0;vertical-align:middle;float:right\" src\u003d\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /\u003e\n    \u003c/a\u003e\n\u003c/p\u003e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424299_266305014",
      "id": "20160720-131940_474698556",
      "dateCreated": "2023-07-16 01:53:44.299",
      "status": "READY"
    },
    {
      "text": "%md\n# Introducción a Spark\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.302",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch1\u003eIntroducción a Spark\u003c/h1\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424302_1122826397",
      "id": "20160628-160644_98292392",
      "dateCreated": "2023-07-16 01:53:44.302",
      "status": "READY"
    },
    {
      "text": "%md\n## Características\n\n### 100x más rápido que Hadoop MapReduce en memoria.\n### 10x más rápido en disco.\n  ![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png)\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.302",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eCaracterísticas\u003c/h2\u003e\n\u003ch3\u003e100x más rápido que Hadoop MapReduce en memoria.\u003c/h3\u003e\n\u003ch3\u003e10x más rápido en disco.\u003c/h3\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424302_879555184",
      "id": "20171013-102503_1459120534",
      "dateCreated": "2023-07-16 01:53:44.302",
      "status": "READY"
    },
    {
      "text": "%md\n### Multiplataforma\n\n* Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, ...)\n* Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png)\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.303",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eMultiplataforma\u003c/h3\u003e\n\u003cul\u003e\n  \u003cli\u003eCorre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, \u0026hellip;)\u003c/li\u003e\n  \u003cli\u003eAcceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\u003cbr/\u003e\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png\" /\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_2022777788",
      "id": "20171013-105605_1380652694",
      "dateCreated": "2023-07-16 01:53:44.303",
      "status": "READY"
    },
    {
      "text": "%md\n### +50 empresas.\n\n### +200 desarrolladores.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png)\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.303",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003e+50 empresas.\u003c/h3\u003e\n\u003ch3\u003e+200 desarrolladores.\u003c/h3\u003e\n\u003cp\u003e\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png\" /\u003e\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_836818994",
      "id": "20171013-112527_2104876012",
      "dateCreated": "2023-07-16 01:53:44.303",
      "status": "READY"
    },
    {
      "title": "Múltiples funcionalidades en una plataforma (Stack unificado)",
      "text": "print(s\"\"\"%html\n\u003cimg src\u003d\"$baseDir/01_intro_spark/unified_stack.png\" alt\u003d\"Drawing\" style\u003d\"width: 60%;\"/\u003e\n\"\"\")\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.303",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cimg src\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/unified_stack.png\" alt\u003d\"Drawing\" style\u003d\"width: 60%;\"/\u003e\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_1772484983",
      "id": "20171013-110124_1830702370",
      "dateCreated": "2023-07-16 01:53:44.303",
      "status": "READY"
    },
    {
      "text": "%md\n## Fácil de usar\n\n* Interface de programación en Scala, Java, Python y R.\n* Notebooks: Zeppelin, Jupiter, ...",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.303",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFácil de usar\u003c/h2\u003e\n\u003cul\u003e\n  \u003cli\u003eInterface de programación en Scala, Java, Python y R.\u003c/li\u003e\n  \u003cli\u003eNotebooks: Zeppelin, Jupiter, \u0026hellip;\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_1931630152",
      "id": "20171013-111851_1940139005",
      "dateCreated": "2023-07-16 01:53:44.303",
      "status": "READY"
    },
    {
      "title": "Word Count (MapReduce)",
      "text": "%md\n```java\npublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper\u003cLongWritable, Text, Text, IntWritable\u003e {\n\n\t\tprivate final static IntWritable one \u003d new IntWritable(1);\n\n\t\tprivate Text word \u003d new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector\u003cText, IntWritable\u003e output, Reporter reporter) throws IOException {\n\n\t\t\tString line \u003d value.toString();\n\n\t\t\tStringTokenizer tokenizer \u003d new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer\u003cText, IntWritable, Text, IntWritable\u003e {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector\u003cText, IntWritable\u003e output, Reporter reporter) throws IOException {\n\n\t\t\tint sum \u003d 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum +\u003d values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf \u003d new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\"wordcount\");\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n```\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.303",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cpre\u003e\u003ccode class\u003d\"java\"\u003epublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper\u0026lt;LongWritable, Text, Text, IntWritable\u0026gt; {\n\n\t\tprivate final static IntWritable one \u003d new IntWritable(1);\n\n\t\tprivate Text word \u003d new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector\u0026lt;Text, IntWritable\u0026gt; output, Reporter reporter) throws IOException {\n\n\t\t\tString line \u003d value.toString();\n\n\t\t\tStringTokenizer tokenizer \u003d new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer\u0026lt;Text, IntWritable, Text, IntWritable\u0026gt; {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector\u0026lt;Text, IntWritable\u0026gt; output, Reporter reporter) throws IOException {\n\n\t\t\tint sum \u003d 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum +\u003d values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf \u003d new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\u0026quot;wordcount\u0026quot;);\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_1935616662",
      "id": "20171011-151944_1744659917",
      "dateCreated": "2023-07-16 01:53:44.303",
      "status": "READY"
    },
    {
      "title": "Word Count (Spark)",
      "text": "%pyspark\n\nlines \u003d sc.textFile(\"README.md\")\n\nwords \u003d lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount \u003d words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:54:51.532",
      "progress": 0,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 14.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_304149368",
      "id": "20201023-001936_119304475",
      "dateCreated": "2023-07-16 01:53:44.303",
      "dateStarted": "2023-07-16 01:54:51.718",
      "dateFinished": "2023-07-16 01:55:45.245",
      "status": "FINISHED"
    },
    {
      "title": "Un poco de Scala",
      "text": "%md\n\n* `lines` es un **array distribuido** de lineas de texto (`RDD[str]`).\n    - una parte del arreglo en cada **nodo del cluster**.\n\n* `lines` tiene el método `flatMap` (línea 6):\n    - `flatMap(lambda line: line.split(\" \"))` toma cada cada elemento del `RDD` (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\n        - `lambda line: line.split(\" \")` es la **función** que toma una linea y la divide en una secuencia de palabras.\n        \n    - Su resultado es un array **distribuido** de palabras (`RDD[str]`).\n    \n* Al resultado de `flatMap` se aplica el método `filter` (línea 7):\n    - `filter(lambda word: word)` saca las palabras que son vacías (pueden aparecer?).\n    - `lambda word: word` es la **función** que pregunta si la palabra es vacía.\n    - `filter` devuelve un `RDD` que se almacena en `words`.\n\n* `words` tiene el método `map` (línea 11):\n    - `map(lambda word: (word,1))` agrega a cada palabra de `words` un `1`.\n    - El resultado es un **arreglo distribuido** de tuplas `RDD[(str, Int)]`.\n    \n* A este `RDD` se le aplica el método `reduceByKey` (línea 12):\n    - `reduceByKey(lambda n,m: n+m)` suma los `1`\u0027s de las palabras iguales (la key es la palabra).\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.304",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": false,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003elines\u003c/code\u003e es un \u003cstrong\u003earray distribuido\u003c/strong\u003e de lineas de texto (\u003ccode\u003eRDD[str]\u003c/code\u003e).\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003euna parte del arreglo en cada \u003cstrong\u003enodo del cluster\u003c/strong\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003elines\u003c/code\u003e tiene el método \u003ccode\u003eflatMap\u003c/code\u003e (línea 6):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n        \u003cp\u003e\u003ccode\u003eflatMap(lambda line: line.split(\u0026quot; \u0026quot;))\u003c/code\u003e toma cada cada elemento del \u003ccode\u003eRDD\u003c/code\u003e (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\u003c/p\u003e\n        \u003cul\u003e\n          \u003cli\u003e\u003ccode\u003elambda line: line.split(\u0026quot; \u0026quot;)\u003c/code\u003e es la \u003cstrong\u003efunción\u003c/strong\u003e que toma una linea y la divide en una secuencia de palabras.\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\n      \u003cp\u003eSu resultado es un array \u003cstrong\u003edistribuido\u003c/strong\u003e de palabras (\u003ccode\u003eRDD[str]\u003c/code\u003e).\u003c/p\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eAl resultado de \u003ccode\u003eflatMap\u003c/code\u003e se aplica el método \u003ccode\u003efilter\u003c/code\u003e (línea 7):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ccode\u003efilter(lambda word: word)\u003c/code\u003e saca las palabras que son vacías (pueden aparecer?).\u003c/li\u003e\n      \u003cli\u003e\u003ccode\u003elambda word: word\u003c/code\u003e es la \u003cstrong\u003efunción\u003c/strong\u003e que pregunta si la palabra es vacía.\u003c/li\u003e\n      \u003cli\u003e\u003ccode\u003efilter\u003c/code\u003e devuelve un \u003ccode\u003eRDD\u003c/code\u003e que se almacena en \u003ccode\u003ewords\u003c/code\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode\u003ewords\u003c/code\u003e tiene el método \u003ccode\u003emap\u003c/code\u003e (línea 11):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ccode\u003emap(lambda word: (word,1))\u003c/code\u003e agrega a cada palabra de \u003ccode\u003ewords\u003c/code\u003e un \u003ccode\u003e1\u003c/code\u003e.\u003c/li\u003e\n      \u003cli\u003eEl resultado es un \u003cstrong\u003earreglo distribuido\u003c/strong\u003e de tuplas \u003ccode\u003eRDD[(str, Int)]\u003c/code\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eA este \u003ccode\u003eRDD\u003c/code\u003e se le aplica el método \u003ccode\u003ereduceByKey\u003c/code\u003e (línea 12):\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ccode\u003ereduceByKey(lambda n,m: n+m)\u003c/code\u003e suma los \u003ccode\u003e1\u003c/code\u003e\u0026rsquo;s de las palabras iguales (la key es la palabra).\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424303_1188061455",
      "id": "20181010-120216_1622145406",
      "dateCreated": "2023-07-16 01:53:44.304",
      "status": "READY"
    },
    {
      "title": "Resultado Word Count Spark",
      "text": "%pyspark\n\nresult \u003d wordCount \\\n    .sortBy((lambda p: p[1]), ascending \u003d False) # ordena por cantidad\n\nlocal_result \u003d result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:56:22.420",
      "progress": 33,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 14.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "from 4\nApache 3\nZeppelin 3\nand 3\nto 3\n* 2\n### 2\nbinary 2\nPlease 2\n[User 2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://localhost:4040/jobs/job?id\u003d0"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id\u003d1"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id\u003d2"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424304_1290879349",
      "id": "20171011-153126_91229243",
      "dateCreated": "2023-07-16 01:53:44.304",
      "dateStarted": "2023-07-16 01:56:22.477",
      "dateFinished": "2023-07-16 01:56:26.714",
      "status": "FINISHED"
    },
    {
      "title": "Run this",
      "text": "%pyspark\n\nuiHost \u003d sc.getConf().get(\"spark.driver.host\")#.getOrElse(\"localhost\")\nuiPort \u003d sc.uiWebUrl.split(\":\")[-1]\n\ntextNabuco \u003d \"\"\"%html\nEjecutar esta celda.\u003cbr\u003e\nHacer un tunel ssh a Nabuco:\u003cbr\u003e\nssh -vCN -L 4040:localhost:{} -l \u0026lt;tu login\u0026gt; nabucodonosor.ccad.unc.edu.ar\u003cbr\u003e\ny ver Spark UI en \n\u003ca href\u003d\"http://{}:{}\"\u003ehttp://{}(host):{}(port)\u003c/a\u003e\n\"\"\".format(uiPort,\"localhost\",\"4040\",\"localhost\",\"4040\")\n\ntextLocal \u003d \"\"\"%html\nEjecutar esta celda y ver Spark UI en \n\u003ca href\u003d\"http://{}:{}\"\u003ehttp://{}(host):{}(port)\u003c/a\u003e\n\"\"\".format(uiHost,uiPort,uiHost,uiPort)\n\nif uiHost \u003d\u003d \"200.16.29.165\":\n    print(textNabuco)\nelse:\n    print(textLocal)\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.304",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": true,
        "fontSize": 15.0,
        "title": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "Ejecutar esta celda y ver Spark UI en \n\u003ca href\u003d\"http://localhost:4040\"\u003ehttp://localhost(host):4040(port)\u003c/a\u003e\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424304_652825211",
      "id": "20171010-193244_2031028749",
      "dateCreated": "2023-07-16 01:53:44.304",
      "status": "READY"
    },
    {
      "title": "Ejercicio 0 (word count)",
      "text": "%md\n* Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \"Add Paragraph\").\n* Copiar el programa `wordcount` anterior en la misma (esta en 2 celdas).\n    - [`shift`]-[`flechas`] para seleccionar.\n    - [`ctrl`]-[`c`] para copiar.\n    - [`ctrl`]-[`v`] para pegar.\n* Modificarlo para leer todas la lineas de los archivos en `./licenses/`\n    - Ayuda: si al método `textFile` se le indica el nombre de un directorio carga todos los archivo del mismo.\n* Ejecute la celda ([`shift`]-[`enter`])\n* Ver la cantidad de tareas en SparkUI\n\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.305",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n  \u003cli\u003eCrear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \u0026ldquo;Add Paragraph\u0026rdquo;).\u003c/li\u003e\n  \u003cli\u003eCopiar el programa \u003ccode\u003ewordcount\u003c/code\u003e anterior en la misma (esta en 2 celdas).\n    \u003cul\u003e\n      \u003cli\u003e[\u003ccode\u003eshift\u003c/code\u003e]-[\u003ccode\u003eflechas\u003c/code\u003e] para seleccionar.\u003c/li\u003e\n      \u003cli\u003e[\u003ccode\u003ectrl\u003c/code\u003e]-[\u003ccode\u003ec\u003c/code\u003e] para copiar.\u003c/li\u003e\n      \u003cli\u003e[\u003ccode\u003ectrl\u003c/code\u003e]-[\u003ccode\u003ev\u003c/code\u003e] para pegar.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eModificarlo para leer todas la lineas de los archivos en \u003ccode\u003e./licenses/\u003c/code\u003e\n    \u003cul\u003e\n      \u003cli\u003eAyuda: si al método \u003ccode\u003etextFile\u003c/code\u003e se le indica el nombre de un directorio carga todos los archivo del mismo.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eEjecute la celda ([\u003ccode\u003eshift\u003c/code\u003e]-[\u003ccode\u003eenter\u003c/code\u003e])\u003c/li\u003e\n  \u003cli\u003eVer la cantidad de tareas en SparkUI\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424304_1701806710",
      "id": "20171010-205347_2087717007",
      "dateCreated": "2023-07-16 01:53:44.305",
      "status": "READY"
    },
    {
      "text": "%md\n\n## Ejecución de programas en Spark\n\n* En [Zeppelin](https://zeppelin.apache.org/) (como lo hacemos ahora)\n* En `pyspark` shell (tambien interactivo)\n* Como programa autónomo\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.305",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eEjecución de programas en Spark\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eEn \u003ca href\u003d\"https://zeppelin.apache.org/\"\u003eZeppelin\u003c/a\u003e (como lo hacemos ahora)\u003c/li\u003e\n\u003cli\u003eEn \u003ccode\u003epyspark\u003c/code\u003e shell (tambien interactivo)\u003c/li\u003e\n\u003cli\u003eComo programa autónomo\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424305_829754548",
      "id": "20171010-202757_196880209",
      "dateCreated": "2023-07-16 01:53:44.305",
      "status": "READY"
    },
    {
      "title": "pyspark shell",
      "text": "%md\n\n* Ir a la terminal donde corre [Zeppelin](https://zeppelin.apache.org/) en [Docker](https://www.docker.com/)\n* Oprimir [`ctrl`]-[`a`] y despues [`c`], para abrir otra terminal\n* Ir a la instalación Spark\n```sh\ncd /opt/spark\n```\n* Arrancar el shell\n```sh\n./bin/pyspark\n```\n* Escribir en shell (apretar `Enter` para ingresar cada línea)\n```python\n\u003e\u003e\u003e lines \u003d sc.textFile(\"README.md\")\n\u003e\u003e\u003e lines.first()\n```\n* Para salir del shell oprima [`ctrl`]-[`d`]",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.305",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n\u003cli\u003eIr a la terminal donde corre \u003ca href\u003d\"https://zeppelin.apache.org/\"\u003eZeppelin\u003c/a\u003e en \u003ca href\u003d\"https://www.docker.com/\"\u003eDocker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eOprimir [\u003ccode\u003ectrl\u003c/code\u003e]-[\u003ccode\u003ea\u003c/code\u003e] y despues [\u003ccode\u003ec\u003c/code\u003e], para abrir otra terminal\u003c/li\u003e\n\u003cli\u003eIr a la instalación Spark\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sh\"\u003ecd /opt/spark\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eArrancar el shell\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sh\"\u003e./bin/pyspark\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eEscribir en shell (apretar \u003ccode\u003eEnter\u003c/code\u003e para ingresar cada línea)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-python\"\u003e\u0026gt;\u0026gt;\u0026gt; lines \u003d sc.textFile(\u0026quot;README.md\u0026quot;)\n\u0026gt;\u0026gt;\u0026gt; lines.first()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003ePara salir del shell oprima [\u003ccode\u003ectrl\u003c/code\u003e]-[\u003ccode\u003ed\u003c/code\u003e]\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424305_591472934",
      "id": "20171011-173126_528319238",
      "dateCreated": "2023-07-16 01:53:44.305",
      "status": "READY"
    },
    {
      "title": "Programa autónomo",
      "text": "%md\n\n* Ir a programa en repo git dentro de [Docker](https://www.docker.com/)\n```sh\ncd /diplodatos_bigdata/prog/word_count\n```\n* Ver programa\n```sh\nless src/main/python/WordCount.py\n```\n  (salir con [`q`])\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.305",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n\u003cli\u003eIr a programa en repo git dentro de \u003ca href\u003d\"https://www.docker.com/\"\u003eDocker\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sh\"\u003ecd /diplodatos_bigdata/prog/word_count\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eVer programa\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sh\"\u003eless src/main/python/WordCount.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(salir con [\u003ccode\u003eq\u003c/code\u003e])\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424305_1924029291",
      "id": "20171011-175259_1199949339",
      "dateCreated": "2023-07-16 01:53:44.305",
      "status": "READY"
    },
    {
      "title": "Ejecucion de programa",
      "text": "%md\n* Ejecutar\n```sh\n/opt/spark/bin/spark-submit --master local[4] \\\n    src/main/python/WordCount.py /opt/spark/licenses/\n```\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 02:15:19.741",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 6.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cul\u003e\n\u003cli\u003eEjecutar\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode class\u003d\"language-sh\"\u003e/opt/spark/bin/spark-submit --master local[4] \\\n    src/main/python/WordCount.py /opt/spark/licenses/\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424305_912299189",
      "id": "20171012-165049_905215351",
      "dateCreated": "2023-07-16 01:53:44.306",
      "status": "READY"
    },
    {
      "title": "Versión Spark en Zeppelin",
      "text": "%pyspark\n\nprint(sc.version)",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.306",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "editorHide": false,
        "fontSize": 14.0,
        "title": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424306_659346319",
      "id": "20170830-114757_1684133948",
      "dateCreated": "2023-07-16 01:53:44.306",
      "status": "READY"
    },
    {
      "text": "%md\n\n### Principales referencias online:\n\n* [Documentación Spark](https://spark.apache.org/docs/2.4.8/)\n* [API Spark Python](https://spark.apache.org/docs/2.4.8/api/python/index.html)\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.306",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003ePrincipales referencias online:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href\u003d\"https://spark.apache.org/docs/2.4.8/\"\u003eDocumentación Spark\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href\u003d\"https://spark.apache.org/docs/2.4.8/api/python/index.html\"\u003eAPI Spark Python\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424306_810727154",
      "id": "20181012-171203_1400816125",
      "dateCreated": "2023-07-16 01:53:44.306",
      "status": "READY"
    },
    {
      "text": "%md\n## Ejercicios MapReduce con Spark",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.306",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eEjercicios MapReduce con Spark\u003c/h2\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424306_238706995",
      "id": "20171016-172908_1510165702",
      "dateCreated": "2023-07-16 01:53:44.306",
      "status": "READY"
    },
    {
      "title": "Ejercicio 1",
      "text": "%md\n\nModifique el programa *word count* siguiente para que cuente la **cantidad de apariciones de cada letra** en el archivo.\n\n* Ayuda: solo hay que modificar la linea 6\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.306",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eModifique el programa \u003cem\u003eword count\u003c/em\u003e siguiente para que cuente la \u003cstrong\u003ecantidad de apariciones de cada letra\u003c/strong\u003e en el archivo.\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eAyuda: solo hay que modificar la linea 6\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424306_1655549904",
      "id": "20171010-202446_178633207",
      "dateCreated": "2023-07-16 01:53:44.306",
      "status": "READY"
    },
    {
      "text": "%pyspark\n\nlines \u003d sc.textFile(\"README.md\")\n\n\n#.flatMap(lambda line: line.split(\" \")) \\\nwords \u003d lines \\\n    .flatMap(lambda line: [*line]) \\\n    .filter(lambda word: word)\n\n\n#MapReduce\nwordCount \u003d words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult \u003d wordCount \\\n    .sortBy((lambda p: p[1]), ascending \u003d False) # ordena por cantidad\n\nlocal_result \u003d result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 02:21:55.893",
      "progress": 66,
      "config": {
        "lineNumbers": true,
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 14.0,
        "editorHide": false,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "e 112\nt 98\n  95\na 85\ni 72\no 72\n/ 67\np 66\ns 62\nn 60\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://localhost:4040/jobs/job?id\u003d11"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id\u003d12"
            },
            {
              "jobUrl": "http://localhost:4040/jobs/job?id\u003d13"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424307_1405904170",
      "id": "20201023-001957_322623490",
      "dateCreated": "2023-07-16 01:53:44.307",
      "dateStarted": "2023-07-16 02:21:55.941",
      "dateFinished": "2023-07-16 02:21:57.027",
      "status": "FINISHED"
    },
    {
      "title": "Ejercicio 2",
      "text": "%md\nCada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n```\n\u003curl\u003e \u003curl link 1\u003e \u003curl link 2\u003e ... \u003curl link n\u003e\n```\n\nBasándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n\n#### Ayuda\n\nA continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.307",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": {},
        "enabled": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eCada línea del archivo \u003ccode\u003e~/diplodatos_bigdata/ds/links_raw.txt\u003c/code\u003e contiene un url de una página web seguido de los links que posee a otras páginas web:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;url\u0026gt; \u0026lt;url link 1\u0026gt; \u0026lt;url link 2\u0026gt; ... \u0026lt;url link n\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBasándose en la utilización de la técnica de \u003cem\u003eMapReduce\u003c/em\u003e que se mostró en el programa \u003ccode\u003eword count\u003c/code\u003e haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\u003c/p\u003e\n\u003ch4\u003eAyuda\u003c/h4\u003e\n\u003cp\u003eA continuación está el comienzo del programa. Falta hacer el \u003cem\u003eMapReduce\u003c/em\u003e y mostrar el resultado.\u003c/p\u003e\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424307_67347754",
      "id": "20171011-175322_1451259292",
      "dateCreated": "2023-07-16 01:53:44.307",
      "status": "READY"
    },
    {
      "text": "%md\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 02:31:25.015",
      "progress": 0,
      "config": {
        "tableHide": false,
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true,
          "completionKey": "TAB",
          "completionSupport": false
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "fontSize": 9.0,
        "editorHide": true,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689474668791_950763472",
      "id": "paragraph_1689474668791_950763472",
      "dateCreated": "2023-07-16 02:31:08.792",
      "dateStarted": "2023-07-16 02:31:14.888",
      "dateFinished": "2023-07-16 02:31:18.811",
      "status": "FINISHED"
    },
    {
      "text": "%pyspark\n\nbaseDir \u003d \"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata\" # llenar con el directorio git\n\nlines \u003d sc.textFile(baseDir + \"/ds/links_raw.txt\")\n\nlinksTo \u003d lines \\\n    .flatMap(lambda l: l.split(\" \")[1:]) # separo los links y tomo los apuntados\n\n# Ahora linksTo tiene las paginas apuntadas\n\n# Completar los ...\n\n# MapReduce\ninvLinkCount \u003d linksTo.map(lambda x: (x,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult \u003d invLinkCount.sortBy((lambda p: p), ascending \u003d False)\n\nlocal_result \u003d result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 02:31:14.626",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "python",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/python",
        "fontSize": 14.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)\n\u001b[0;32m/tmp/ipykernel_609/2323021292.py\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# MapReduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0minvLinkCount\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mlinksTo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 14\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0minvLinkCount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduceByKey\u001b[0;34m(self, func, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027a\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u0027b\u0027\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m         \"\"\"\n\u001b[0;32m-\u003e 1625\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombineByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduceByKeyLocally\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mcombineByKey\u001b[0;34m(self, createCombiner, mergeValue, mergeCombiners, numPartitions, partitionFunc)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \"\"\"\n\u001b[1;32m   1852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumPartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1853\u001b[0;31m             \u001b[0mnumPartitions\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_defaultReducePartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mserializer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36m_defaultReducePartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2261\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2263\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2517\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value \u003d get_return_value(\n\u001b[0;32m-\u003e 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\n\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n\n\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o515.partitions.\n: java.io.IOException: No FileSystem for scheme: https\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424307_1075964987",
      "id": "20191121-184701_1405603118",
      "dateCreated": "2023-07-16 01:53:44.308",
      "dateStarted": "2023-07-16 02:30:53.695",
      "dateFinished": "2023-07-16 02:30:54.138",
      "status": "ERROR"
    },
    {
      "title": "FIN",
      "text": "//val baseDir\u003d\"https://git.cs.famaf.unc.edu.ar/dbarsotti/diplodatos_bigdata/raw/master/clases\"\nval baseDir\u003d\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\"\n\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n\u003cscript\u003e\n    var heads \u003d document.getElementsByTagName(\u0027h2\u0027);\n    var numHeads \u003d heads.length;\n    var inner \u003d \"\";\n    var i \u003d 0;\n    var j \u003d 0;\n    while (i \u003c numHeads){\n        inner \u003d heads[i].innerHTML;\n        if (inner.search(\".-\") !\u003d -1 ) {\n            j++;\n            heads[i].innerHTML \u003d inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n\u003c/script\u003e\n\"\"\")",
      "user": "anonymous",
      "dateUpdated": "2023-07-16 01:53:44.308",
      "progress": 0,
      "config": {
        "tableHide": true,
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "fontSize": 9.0,
        "title": true,
        "results": [
          {
            "graph": {
              "mode": "table",
              "height": 300.0,
              "optionOpen": false,
              "keys": [],
              "values": [],
              "groups": [],
              "scatter": {}
            }
          }
        ],
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cscript\u003e\n    var heads \u003d document.getElementsByTagName(\u0027h2\u0027);\n    var numHeads \u003d heads.length;\n    var inner \u003d \"\";\n    var i \u003d 0;\n    var j \u003d 0;\n    while (i \u003c numHeads){\n        inner \u003d heads[i].innerHTML;\n        if (inner.search(\".-\") !\u003d -1 ) {\n            j++;\n            heads[i].innerHTML \u003d inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n\u003c/script\u003e\nbaseDir: String \u003d https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1689472424308_521864348",
      "id": "20160712-175904_2058049512",
      "dateCreated": "2023-07-16 01:53:44.308",
      "status": "READY"
    }
  ],
  "name": "Clase 01 - Introducción a Spark",
  "id": "2J7ZKNQC5",
  "defaultInterpreterGroup": "spark",
  "version": "0.10.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {}
}